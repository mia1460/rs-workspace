# python scripts/analyze_data_distribution.py /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-Pure/data/log_standard_4_08_to_4_21_pure.csv /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-Pure/data/log_standard_4_22_to_5_08_pure.csv --delta_t 3600000 --output_dir /home/xieminhui/yinj/workplace/recsys-examples/data/data_distribution/kuairand_pure --plot
# python scripts/analyze_data_distribution.py /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_08_to_4_21_1k.csv /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_22_to_5_08_1k.csv --delta_t 3600000 --output_dir /home/xieminhui/yinj/workplace/recsys-examples/data/data_distribution/kuairand_1k --plot
# python scripts/analyze_data_distribution.py /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-27K/data/log_standard_4_08_to_4_21_27k_part1.csv /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-27K/data/log_standard_4_08_to_4_21_27k_part2.csv /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-27K/data/log_standard_4_22_to_5_08_27k_part1.csv /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-27K/data/log_standard_4_22_to_5_08_27k_part2.csv --delta_t 3600000 --output_dir /home/xieminhui/yinj/workplace/recsys-examples/data/data_distribution/kuairand_27k --plot
# python scripts/analyze_data_distribution.py /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_08_to_4_21_1k.csv /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_22_to_5_08_1k.csv --delta_t 60000 --output_dir /home/xieminhui/yinj/workplace/recsys-examples/data/data_distribution/kuairand_1k --plot
# PYTHONUNBUFFERED=1 python -u scripts/analyze_data_distribution.py   /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_08_to_4_21_1k.csv   /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_22_to_5_08_1k.csv   --delta_t 60000   --output_dir /home/xieminhui/yinj/workplace/recsys-examples/data/data_distribution/kuairand_1k   --plot   > /home/xieminhui/yinj/workplace/recsys-examples/data/running_log/kuairand_1k_1min.log 2>&1
# PYTHONUNBUFFERED=1 python -u scripts/analyze_data_distribution.py /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-27K/data/log_standard_4_08_to_4_21_27k_part1.csv /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-27K/data/log_standard_4_08_to_4_21_27k_part2.csv /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-27K/data/log_standard_4_22_to_5_08_27k_part1.csv /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-27K/data/log_standard_4_22_to_5_08_27k_part2.csv --delta_t 3600000 --output_dir /home/xieminhui/yinj/workplace/recsys-examples/data/data_distribution/kuairand_27k --plot   > /home/xieminhui/yinj/workplace/recsys-examples/data/running_log/kuairand_27k_1h.log 2>&1
# PYTHONUNBUFFERED=1 python -u scripts/analyze_data_distribution.py   /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_08_to_4_21_1k.csv   /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_22_to_5_08_1k.csv   --delta_t 3600000   --output_dir /home/xieminhui/yinj/workplace/recsys-examples/data/data_distribution/kuairand_1k --plot --force_analyze   > /home/xieminhui/yinj/workplace/recsys-examples/data/running_log/kuairand_1k_1h.log 2>&1
# PYTHONUNBUFFERED=1 python -u scripts/analyze_data_distribution.py   /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_08_to_4_21_1k.csv   /home/xieminhui/yinj/workplace/recsys-examples/examples/hstu/tmp_data/KuaiRand-1K/data/log_standard_4_22_to_5_08_1k.csv   --delta_t 60000   --output_dir /home/xieminhui/yinj/workplace/recsys-examples/data/data_distribution/kuairand_1k --plot --force_analyze   > /home/xieminhui/yinj/workplace/recsys-examples/data/running_log/kuairand_1k_1min.log 2>&1

import pandas as pd
import numpy as np
import argparse
import matplotlib.pyplot as plt
from datetime import datetime
import json
import os
from collections import defaultdict
import matplotlib as mpl

def parse_arguments():
    parser = argparse.ArgumentParser(
        description='User Interaction Data Analysis and Visualization',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    
    parser.add_argument('input_files', nargs='+', 
                      help='Path to input CSV file(s) (support multiple files)')
    parser.add_argument('--delta_t', type=int, default=60000,
                      help='Time window size in milliseconds')
    parser.add_argument('--output_dir', default='./results',
                      help='Output directory path')
    parser.add_argument('--output_prefix', default='interaction_stats',
                      help='Prefix for output filenames')
    parser.add_argument('--time_col', default='time_ms',
                      help='Timestamp column name')
    parser.add_argument('--user_col', default='user_id',
                      help='User ID column name')
    parser.add_argument('--force_analyze', action='store_true',
                      help='Force re-run data analysis (skip file check)')
    parser.add_argument('--plot', action='store_true',
                      help='Generate visualization plots')
    
    return parser.parse_args()

def load_and_combine_files(file_paths):
    """Load and combine multiple CSV files with validation"""
    dfs = []
    for file_path in file_paths:
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"Input file not found: {file_path}")
        
        print(f"Loading {os.path.basename(file_path)}...", end=' ', flush=True)
        df = pd.read_csv(file_path)
        dfs.append(df)
        print(f"loaded {len(df):,} records")
    
    combined_df = pd.concat(dfs, ignore_index=True).sort_values(time_col)
    print(f"\nCombined dataset: {len(combined_df):,} total records")
    return combined_df, total_records

def calculate_metrics(df, user_col, cumulative_data=None):
    """Calculate key metrics including per-window averages"""
    metrics = {
        'total_interactions': len(df),
        'unique_users': df[user_col].nunique(),
        'avg_interactions_per_user': len(df) / df[user_col].nunique() if df[user_col].nunique() > 0 else 0,
        'interaction_counts': df[user_col].value_counts().to_dict(),
    }
    metrics['max_interactions_per_user'] = max(metrics['interaction_counts'].values()) if metrics['interaction_counts'] else 0

    # æ–°å¢žï¼šè®¡ç®—å¹³å‡ç”¨æˆ·æ€»äº¤äº’æ¬¡æ•°
    if cumulative_data is None:
        user_total_interactions = df[user_col].value_counts()
        metrics['avg_total_interactions_per_user'] = user_total_interactions.mean()
        # æ–°å¢žï¼šè®¡ç®—æ‰€æœ‰ç”¨æˆ·ä¸­æ€»äº¤äº’æ¬¡æ•°çš„æœ€å¤§å€¼
        metrics['max_total_interactions_per_user'] = user_total_interactions.max() if not user_total_interactions.empty else 0
    else:
        metrics['avg_total_interactions_per_user'] = 0
        metrics['max_total_interactions_per_user'] = 0 # Initialize even if no cumulative data
    return metrics

def analyze_data(df, delta_t_ms, time_col, user_col):
    """Analyze data with progress display"""
    print("\nðŸ” Starting data analysis...")
    
    # Data preprocessing
    print("â³ Preprocessing data (time conversion, sorting)...", end=' ', flush=True)
    df['timestamp'] = pd.to_datetime(df[time_col], unit='ms')
    df = df.sort_values('timestamp')
    min_time = df['timestamp'].min()
    df['time_since_start'] = (df['timestamp'] - min_time).dt.total_seconds() * 1000
    total_records = len(df)
    print(f"Done! Total {total_records:,} records")
    
    # Initialize time windows
    max_time = df['time_since_start'].max()
    time_windows = np.arange(0, max_time + delta_t_ms, delta_t_ms)
    total_windows = len(time_windows)
    print(f"â° Will analyze {total_windows} time windows ({delta_t_ms/1000:.1f}s each)")
    
    window_stats = []
    cumulative_stats = []
    prev_users = 0
    
    # Progress display configuration
    progress_interval = max(1, total_windows // 10)  # Show progress at least 10 times
    
    print("\nðŸ“Š Analysis progress:")
    for i, window_end in enumerate(time_windows):
        # Window statistics
        window_mask = ((df['time_since_start'] > (window_end - delta_t_ms)) & 
                      (df['time_since_start'] <= window_end))
        window_data = df[window_mask]
        
        # Cumulative statistics
        cumulative_data = df[df['time_since_start'] <= window_end]
        
        # Calculate metrics
        window_metrics = calculate_metrics(window_data, user_col, cumulative_data)
        cumulative_metrics = calculate_metrics(cumulative_data, user_col)
        
        # Record results
        window_stats.append({'window_end': window_end, **window_metrics})
        cumulative_stats.append({'time_point': window_end, **cumulative_metrics})
        
        # Progress display
        if (i % progress_interval == 0) or (i == total_windows - 1):
            new_users = cumulative_metrics['unique_users'] - prev_users
            progress = (i + 1) / total_windows * 100
            time_elapsed = window_end / 1000
            print(
                f"â–{progress:.0f}% â–Time: {time_elapsed:.1f}s â–"
                f"Window interactions: {window_metrics['total_interactions']:4d} â–"
                f"Window users: {window_metrics['unique_users']:3d} â–"
                f"Window avg interactions: {window_metrics['avg_interactions_per_user']:4.1f} â–" 
                f"New users: {new_users:3d} â–"
                f"Total users: {cumulative_metrics['unique_users']:5d}"
            )
            prev_users = cumulative_metrics['unique_users']
    
    # Final statistics
    print("\nâœ… Analysis completed!")
    final_stats = cumulative_stats[-1]
    print(f"â€¢ Total time span: {max_time/1000:.1f} seconds")
    print(f"â€¢ Total users: {final_stats['unique_users']:,}")
    print(f"â€¢ Total interactions: {final_stats['total_interactions']:,}")
    print(f"â€¢ Avg interactions/user: {final_stats['avg_interactions_per_user']:.1f}")
    
    return pd.DataFrame(window_stats), pd.DataFrame(cumulative_stats)

def visualize_results(window_df, cumulative_df, output_path):
    """Enhanced visualization with larger fonts and 2x3 layout"""
    # ======================
    # 1. å­—ä½“å…¨å±€è®¾ç½®
    # ======================
    plt.style.use('seaborn-v0_8')
    mpl.rcParams.update({
        'font.size': 14,          # é€‚å½“å‡å°å­—å·ä»¥é€‚åº”æ›´å¤šå›¾è¡¨
        'axes.titlesize': 20,     # æ ‡é¢˜å­—å·
        'axes.labelsize': 18,     # åæ ‡è½´æ ‡ç­¾
        'xtick.labelsize': 16,    # Xè½´åˆ»åº¦
        'ytick.labelsize': 16,    # Yè½´åˆ»åº¦
        'legend.fontsize': 16,    # å›¾ä¾‹å­—å·
    })

    # ======================
    # 2. åˆ›å»ºç”»å¸ƒï¼ˆè°ƒæ•´ä¸º2è¡Œ3åˆ—ï¼‰
    # ======================
    fig = plt.figure(figsize=(24, 16), dpi=120)  # å¢žåŠ å®½åº¦ä»¥é€‚åº”3åˆ—
    fig.suptitle('User Interaction Analysis', fontsize=24, y=0.98)

    # è®¡ç®—æ¡å½¢å®½åº¦ï¼ˆæ‰€æœ‰æ¡å½¢å›¾ä¿æŒä¸€è‡´ï¼‰
    bar_width = window_df['window_end'].diff().mean()/1000*0.6  # ç¨å¾®å‡å°å®½åº¦

    # ======================
    # 3. å­å›¾ç»˜åˆ¶ï¼ˆ2è¡Œ3åˆ—å¸ƒå±€ï¼‰
    # ======================
    # 3.1 ç´¯è®¡ç”¨æˆ·æ•°ï¼ˆç¬¬1è¡Œç¬¬1åˆ—ï¼‰
    ax1 = fig.add_subplot(2, 3, 1)
    ax1.plot(cumulative_df['time_point']/1000, 
            cumulative_df['unique_users'], 
            'b-', linewidth=2.5)
    ax1.set_title('Cumulative Unique Users', pad=15)
    ax1.set_xlabel('Time (seconds)', fontsize=16)
    ax1.set_ylabel('User Count', fontsize=16)
    ax1.grid(True, alpha=0.3)
    
    # 3.2 çª—å£ç”¨æˆ·æ•°ï¼ˆç¬¬1è¡Œç¬¬2åˆ—ï¼‰
    ax2 = fig.add_subplot(2, 3, 2)
    ax2.bar(window_df['window_end']/1000, 
           window_df['unique_users'],
           width=bar_width,
           color='#FF7F0E', edgecolor='none')
    ax2.set_title('Unique Users per Window', pad=15)
    ax2.set_xlabel('Time (seconds)', fontsize=16)
    ax2.set_ylabel('User Count', fontsize=16)
    ax2.grid(True, axis='y', alpha=0.3)
    
    # 3.3 çª—å£äº¤äº’é‡ï¼ˆç¬¬1è¡Œç¬¬3åˆ—ï¼‰
    ax3 = fig.add_subplot(2, 3, 3)
    ax3.bar(window_df['window_end']/1000, 
           window_df['total_interactions'],
           width=bar_width,
           color='#2CA02C', edgecolor='none')
    ax3.set_title('Interactions per Window', pad=15)
    ax3.set_xlabel('Time (seconds)', fontsize=16)
    ax3.set_ylabel('Interaction Count', fontsize=16)
    ax3.grid(True, axis='y', alpha=0.3)
    
    # 3.4 å¹³å‡äº¤äº’æ¬¡æ•°ï¼ˆç¬¬2è¡Œç¬¬1åˆ—ï¼‰
    ax4 = fig.add_subplot(2, 3, 4)
    ax4.bar(window_df['window_end']/1000, 
            window_df['avg_interactions_per_user'],
            width=bar_width, 
            color='#D62728', edgecolor='none', label='Average Total Interactions')
    # Add line for max_interactions_per_user (within window)
    max_interaction_per_user_values = window_df['max_interactions_per_user']
    time_points_ax4 = window_df['window_end']/1000
    
    ax4.plot(time_points_ax4,
             max_interaction_per_user_values,
             'k--', linewidth=1.5, label='Max Interactions per User (Window)')
    
    # æ‰¾åˆ°max_interactions_per_userçš„æœ€é«˜ç‚¹å¹¶æ ‡æ³¨
    max_idx_ax4 = max_interaction_per_user_values.idxmax()
    max_time_ax4 = time_points_ax4.loc[max_idx_ax4]
    max_value_ax4 = max_interaction_per_user_values.loc[max_idx_ax4]
    ax4.annotate(f'{int(max_value_ax4)}', 
                 xy=(max_time_ax4, max_value_ax4), 
                 xytext=(max_time_ax4 + 0.05 * (time_points_ax4.max() - time_points_ax4.min()), max_value_ax4 + 0.05 * max_value_ax4), # è°ƒæ•´æ–‡æœ¬ä½ç½®
                 arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=5),
                 fontsize=12, color='black')

    ax4.set_title('Avg & Max Interactions per User per Window', pad=15)
    ax4.set_xlabel('Time (seconds)', fontsize=16)
    ax4.set_ylabel('Per User Window Interactions Count', fontsize=16)
    ax4.grid(True, alpha=0.3)
    ax4.legend() 

    # 3.5 æ–°å¢žï¼šå¹³å‡ç”¨æˆ·æ€»äº¤äº’æ¬¡æ•°ï¼ˆç¬¬2è¡Œç¬¬2åˆ—ï¼‰
    ax5 = fig.add_subplot(2, 3, 5)
    ax5.bar(cumulative_df['time_point']/1000,
            cumulative_df['avg_total_interactions_per_user'],
            width=bar_width, 
            color='#D62728', edgecolor='none', label='Average Total Interactions') # æ·»åŠ label
    
    # æ·»åŠ æŠ˜çº¿ï¼Œä½“çŽ°æ‰€æœ‰ç”¨æˆ·ä¸­æ€»äº¤äº’æ¬¡æ•°çš„æœ€å¤§å€¼
    max_total_interactions_values = cumulative_df['max_total_interactions_per_user']
    time_points_ax5 = cumulative_df['time_point']/1000

    ax5.plot(time_points_ax5,
             max_total_interactions_values,
             'k--', linewidth=1.5, label='Max Total Interactions') # é»‘è‰²è™šçº¿
    
    # æ‰¾åˆ°max_total_interactions_per_userçš„æœ€é«˜ç‚¹å¹¶æ ‡æ³¨
    max_idx_ax5 = max_total_interactions_values.idxmax()
    max_time_ax5 = time_points_ax5.loc[max_idx_ax5]
    max_value_ax5 = max_total_interactions_values.loc[max_idx_ax5]
    ax5.annotate(f'{int(max_value_ax5)}', 
                 xy=(max_time_ax5, max_value_ax5), 
                 xytext=(max_time_ax5 + 0.05 * (time_points_ax5.max() - time_points_ax5.min()), max_value_ax5 + 0.05 * max_value_ax5), # è°ƒæ•´æ–‡æœ¬ä½ç½®
                 arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=5),
                 fontsize=12, color='black')

    ax5.set_title('Avg & Max Total Interactions per User', pad=15)
    ax5.set_xlabel('Time (seconds)', fontsize=16)
    ax5.set_ylabel('Per User Total Interactions Count', fontsize=16)
    ax5.grid(True, alpha=0.3)
    ax5.legend() 

    # 3.6 ä¿ç•™ä¸ºç©ºï¼ˆç¬¬2è¡Œç¬¬3åˆ—ï¼‰
    ax6 = fig.add_subplot(2, 3, 6)
    ax6.text(0.5, 0.5, 'Additional Metrics\n(To Be Determined)',
            horizontalalignment='center',
            verticalalignment='center',
            transform=ax6.transAxes,
            fontsize=16)
    ax6.set_title('Reserved for Future Use', pad=15)
    ax6.axis('off')  # éšè—åæ ‡è½´

    # ======================
    # 4. ç»Ÿä¸€è®¾ç½®å’Œè¾“å‡ºä¼˜åŒ–
    # ======================
    # è°ƒæ•´å­å›¾é—´è·
    plt.tight_layout(pad=3.0, h_pad=3.0, w_pad=2.0)
    
    # ä¿å­˜å›¾åƒ
    plt.savefig(output_path, dpi=150, bbox_inches='tight', facecolor='white')
    plt.close()
    print(f"ðŸ“Š Visualization saved with 2x3 layout: {output_path}")

    return output_path

def save_json_distributions(df, output_path):
    """Save distribution data as structured JSON"""
    dist_data = []
    for _, row in df.iterrows():
        dist_data.append({
            'time_point': row['time_point'],
            'user_distribution': {
                'total_users': row['unique_users'],
                'interaction_dist': {
                    str(k): int(v) for k, v in 
                    sorted(row['interaction_counts'].items())
                }
            }
        })
    
    with open(output_path, 'w') as f:
        json.dump(dist_data, f, indent=2)

def check_existing_files(output_dir, output_prefix, delta_t, total_records):
    """Check if analysis files already exist with new naming format"""
    delta_s = delta_t // 1000
    rec_k = total_records // 1000  # Convert to thousands
    file_suffix = f"{delta_s}s_{rec_k}k"
    
    files = {
        'window': os.path.join(output_dir, f"{output_prefix}_window_{file_suffix}.csv"),
        'cumulative': os.path.join(output_dir, f"{output_prefix}_cumulative_{file_suffix}.csv"),
        'json': os.path.join(output_dir, f"{output_prefix}_distributions_{file_suffix}.json"),
        'plot': os.path.join(output_dir, f"{output_prefix}_plot_{file_suffix}.png")
    }
    
    # Check if all required files exist
    required_files = [files['window'], files['cumulative'], files['json']]
    all_exist = all(os.path.exists(f) for f in required_files)
    return files, all_exist

def load_existing_data(output_files):
    """Load existing analysis results"""
    print("Loading existing analysis files...")
    window_df = pd.read_csv(output_files['window'])
    cumulative_df = pd.read_csv(output_files['cumulative'])
    return window_df, cumulative_df

def main():
    args = parse_arguments()
    os.makedirs(args.output_dir, exist_ok=True)
    
    # å¢žå¼ºçš„å¯åŠ¨ä¿¡æ¯ï¼ˆä¿æŒä¸å˜ï¼‰
    print(f"\n{'='*50}")
    print(f"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Input file(s):")
    for i, file_path in enumerate(args.input_files, 1):
        print(f"  [{i}] {os.path.basename(file_path)}")
    print(f"Time window: {args.delta_t}ms ({args.delta_t/1000:.1f}s)")
    print(f"Output directory: {os.path.abspath(args.output_dir)}")
    print(f"Output prefix: {args.output_prefix}")
    print(f"{'='*50}\n")

    try:
        # ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ•°æ®å¹¶è®¡ç®—æ€»è®°å½•æ•°
        print("\nðŸ“‚ Loading input files:")
        dfs = []
        total_records = 0
        for file_path in args.input_files:
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Input file not found: {file_path}")
            
            df = pd.read_csv(file_path)
            record_count = len(df)
            dfs.append(df)
            total_records += record_count
            print(f"  - {os.path.basename(file_path)}: {record_count:,} records")
        
        combined_df = pd.concat(dfs, ignore_index=True).sort_values(args.time_col)
        print(f"\nðŸ“Š Combined dataset: {total_records:,} total records")

        # ç¬¬äºŒæ­¥ï¼šç”Ÿæˆå¸¦è®°å½•æ•°çš„æ–‡ä»¶å
        delta_s = args.delta_t // 1000
        rec_k = total_records // 1000  # è½¬æ¢ä¸ºåƒå•ä½
        file_suffix = f"{delta_s}s_{rec_k}k"
        
        output_files = {
            'window': os.path.join(args.output_dir, f"{args.output_prefix}_window_{file_suffix}.csv"),
            'cumulative': os.path.join(args.output_dir, f"{args.output_prefix}_cumulative_{file_suffix}.csv"),
            'json': os.path.join(args.output_dir, f"{args.output_prefix}_distributions_{file_suffix}.json"),
            'plot': os.path.join(args.output_dir, f"{args.output_prefix}_plot_{file_suffix}.png")
        }

        # ç¬¬ä¸‰æ­¥ï¼šæ£€æŸ¥æ˜¯å¦è·³è¿‡åˆ†æžï¼ˆä¿æŒåŽŸæœ‰æ¸…æ™°æç¤ºï¼‰
        if not args.force_analyze:
            existing_files, all_exist = check_existing_files(args.output_dir, args.output_prefix, args.delta_t, total_records)
            if all_exist:
                print("\nðŸ” Found existing analysis results:")
                for file_type, path in existing_files.items():
                    print(f"  - {file_type.ljust(10)}: {os.path.basename(path)}")
                window_df, cumulative_df = load_existing_data(existing_files)
                print("\nâœ… Using existing results (use --force_analyze to re-run analysis)")
            else:
                print("\nâ„¹ï¸ No complete analysis results found, proceeding with fresh analysis...")
                args.force_analyze = True

        # ç¬¬å››æ­¥ï¼šè¿è¡Œåˆ†æžï¼ˆä¿æŒåŽŸæœ‰ç»“æž„ï¼‰
        if args.force_analyze:
            window_df, cumulative_df = analyze_data(combined_df, args.delta_t, args.time_col, args.user_col)
            
            # ä¿å­˜ç»“æžœï¼ˆå¢žå¼ºæ–‡ä»¶åæç¤ºï¼‰
            print("\nðŸ’¾ Saving analysis results:")
            window_df.to_csv(output_files['window'], index=False)
            print(f"  - Window stats: {os.path.basename(output_files['window'])}")
            
            cumulative_df.to_csv(output_files['cumulative'], index=False)
            print(f"  - Cumulative stats: {os.path.basename(output_files['cumulative'])}")
            
            save_json_distributions(cumulative_df, output_files['json'])
            print(f"  - Distribution data: {os.path.basename(output_files['json'])}")
            
            print("âœ… Analysis completed and saved")

        # ç¬¬äº”æ­¥ï¼šå¯è§†åŒ–ï¼ˆä¿æŒåŽŸæœ‰æ¸…æ™°æç¤ºï¼‰
        if args.plot:
            print("\nðŸŽ¨ Generating visualization...")
            plot_path = visualize_results(window_df, cumulative_df, output_files['plot'])
            print(f"  - Saved to: {os.path.basename(plot_path)}")

        # æœ€ç»ˆæ‘˜è¦ï¼ˆå¢žå¼ºè®°å½•æ•°æ˜¾ç¤ºï¼‰
        print(f"\n{'='*50}")
        print("Operation Summary".center(50))
        print(f"{'='*50}")
        print(f"ðŸ“… Time range analyzed: {cumulative_df['time_point'].iloc[-1]/1000:.1f}s")
        print(f"ðŸ“Š Total records processed: {total_records:,}")
        print(f"ðŸ‘¥ Unique users: {cumulative_df['unique_users'].iloc[-1]:,}")
        print(f"ðŸ”„ Total interactions: {cumulative_df['total_interactions'].iloc[-1]:,}")
        
        if args.plot:
            print(f"\nðŸŽ¨ Visualization file: {os.path.abspath(output_files['plot'])}")
        print(f"\nðŸ’¾ Results location: {os.path.abspath(args.output_dir)}")
        print(f"{'='*50}")

    except Exception as e:
        print(f"\n{'âŒ'*20}")
        print("Processing Failed!".center(40))
        print(f"{'âŒ'*20}")
        print(f"Error: {str(e)}")
        print(f"\nDebug info:")
        print(f"- Input files: {[os.path.basename(f) for f in args.input_files]}")
        print(f"- Output dir: {args.output_dir}")
        if 'total_records' in locals():
            print(f"- Loaded records: {total_records:,}")
        raise


if __name__ == "__main__":
    main()