[2025-08-13 13:43:42] [DEBUG] __main__ (pretrain_gr_ranking.py:43): successfully init logging
/usr/local/lib/python3.12/dist-packages/rapids_dask_dependency/dask_loader.py:36: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  return importlib.import_module(spec.name)
NOTE! Installing ujson may make loading annotations faster.
[2025-08-13 13:43:50.327457] distributed env initialization done. Free cuda memory: 23622.44 MB
RankingGR(
  (_embedding_collection): ShardedEmbedding(
    (_model_parallel_embedding_collection): EmbeddingCollection(
      (embeddings): ModuleDict(
        (movie_id): Embedding(10000000, 128)
        (user_id): Embedding(10000000, 128)
      )
    )
    (_data_parallel_embedding_collection): EmbeddingCollection(
      (embeddings): ModuleDict(
        (action_weights): Embedding(11, 128)
      )
    )
  )
  (_hstu_block): HSTUBlock(
    (_preprocessor): HSTUBlockPreprocessor(
      (_positional_encoder): HSTUPositionalEncoder()
    )
    (_postprocessor): HSTUBlockPostprocessor()
    (_attention_layers): ModuleList(
      (0): HSTULayer(
        (_output_ln_dropout_mul): TPLayerNormMulDropout()
        (_linear_uvqk): TEColumnParallelLinear(in_features=128, out_features=2048, bias=True, TP=1)
        (_linear_proj): TERowParallelLinear(in_features=512, out_features=128, bias=False, TP=1)
        (_attn_func): FusedHSTUAttention()
      )
    )
  )
  (_mlp): MLP(
    (_mlp): Sequential(
      (0): Linear(in_features=128, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10, bias=True)
      (3): Identity()
    )
  )
  (_loss_module): MultiTaskLossModule(
    (_loss_modules): CrossEntropyLoss()
  )
  (_metric_module): MultiClassificationTaskMetric(
    (_eval_metrics_modules): ModuleList(
      (0): MulticlassAUROC()
    )
  )
)


table name |          | memory(MB) |             |         | hbm(MB)/cuda:0 |             |          |  dram(MB) |            
---------- | -------- | ---------- | ----------- | ------- | -------------- | ----------- | -------- | --------- | -----------
           | total    | embedding  | optim_state | total   | embedding      | optim_state | total    | embedding | optim_state
---------- | -------- | ---------- | ----------- | ------- | -------------- | ----------- | -------- | --------- | -----------
movie_id   | 25165824 | 8388608    | 16777216    | 5000000 | 1666666        | 3333333     | 20165824 | 6721941   | 13443882   
user_id    | 25165824 | 8388608    | 16777216    | 5000000 | 1666666        | 3333333     | 20165824 | 6721941   | 13443882   
[2025-08-13 13:44:06] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-13 13:44:11.627011] [SequenceDataset] Filtered samples with short sequences: 4485 removed, 134008 remaining.
[2025-08-13 13:44:16.243269] [SequenceDataset] Filtered samples with short sequences: 4485 removed, 134008 remaining.
[2025-08-13 13:44:16.244421] model initialization done, start training. Free cuda memory: 14448.44 MB
[2025-08-13 13:44:16.245331] =========Training without time window============
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-08-13 13:44:24.968795] [train] [iter 9, tokens 25600, elapsed_time 8017.74 ms, achieved FLOPS 0.23 TFLOPS]: loss 2.010956
[2025-08-13 13:44:39.517072] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.634451
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-08-13 13:44:39.877240] [train] [iter 19, tokens 25600, elapsed_time 15613.65 ms, achieved FLOPS 0.12 TFLOPS]: loss 1.805194
[2025-08-13 13:44:48.572268] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.637032
[2025-08-13 13:44:48.954929] [train] [iter 29, tokens 25600, elapsed_time 9077.67 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.739561
[2025-08-13 13:44:58.502330] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.704558
[2025-08-13 13:44:58.920546] [train] [iter 39, tokens 25600, elapsed_time 9964.78 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.640844
[2025-08-13 13:45:07.779760] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.729720
[2025-08-13 13:45:08.250926] [train] [iter 49, tokens 25600, elapsed_time 9331.02 ms, achieved FLOPS 0.29 TFLOPS]: loss 1.668421
[2025-08-13 13:45:17.086301] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.745780
[2025-08-13 13:45:17.519843] [train] [iter 59, tokens 25600, elapsed_time 9268.86 ms, achieved FLOPS 0.22 TFLOPS]: loss 1.657074
[2025-08-13 13:45:27.807703] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.754008
[2025-08-13 13:45:28.220076] [train] [iter 69, tokens 25600, elapsed_time 10700.27 ms, achieved FLOPS 0.15 TFLOPS]: loss 1.637218
[2025-08-13 13:45:37.136012] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.762287
[2025-08-13 13:45:37.536821] [train] [iter 79, tokens 25600, elapsed_time 9316.59 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.600544
[2025-08-13 13:45:46.368521] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.768166
[2025-08-13 13:45:46.820324] [train] [iter 89, tokens 25600, elapsed_time 9283.34 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.648939
[2025-08-13 13:45:57.208526] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.773839
[2025-08-13 13:45:57.630180] [train] [iter 99, tokens 25600, elapsed_time 10810.05 ms, achieved FLOPS 0.14 TFLOPS]: loss 1.660765
[2025-08-13 13:46:07.822828] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.779087
[2025-08-13 13:46:07.828815] ==================================================
[2025-08-13 13:46:07.828855] ==> SAVING CHECKPOINTS TO: /workspace/recsys-shared_data/ckpts/ml-20m/ranking/2025_08_13-13_43_49/final-iter100
[2025-08-13 13:46:07.828866] ==================================================
[yinj] Saving model type: <class 'torchrec.distributed.model_parallel.DistributedModelParallel'>
[yinj] Saving unwrapped_module type: <class 'model.ranking_gr.RankingGR'>
[2025-08-13 13:46:07.829208] dynamic module save dir /workspace/recsys-shared_data/ckpts/ml-20m/ranking/2025_08_13-13_43_49/final-iter100/dynamicemb_module
[yinj] [RANK 0] world_size = 1
DynamicEmb dump table movie_id from module _model_parallel_embedding_collection success!
[yinj] [RANK 0] world_size = 1
DynamicEmb dump table user_id from module _model_parallel_embedding_collection success!
[2025-08-13 13:46:08.168277] torch module save dir /workspace/recsys-shared_data/ckpts/ml-20m/ranking/2025_08_13-13_43_49/final-iter100/torch_module
[2025-08-13 13:46:08.354269] ==================================================
[2025-08-13 13:46:08.354340] ==> CHECKPOINTS SAVED SUCCESSFULLY âœ…
[2025-08-13 13:46:08.354359] ==================================================
[rank0]:[W813 13:46:10.918413169 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
