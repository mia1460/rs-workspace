[2025-08-13 14:13:28] [DEBUG] __main__ (pretrain_gr_ranking.py:43): successfully init logging
/usr/local/lib/python3.12/dist-packages/rapids_dask_dependency/dask_loader.py:36: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  return importlib.import_module(spec.name)
[2025-08-13 14:13:36.680537] distributed env initialization done. Free cuda memory: 23622.44 MB
RankingGR(
  (_embedding_collection): ShardedEmbedding(
    (_model_parallel_embedding_collection): EmbeddingCollection(
      (embeddings): ModuleDict(
        (movie_id): Embedding(10000000, 128)
        (user_id): Embedding(10000000, 128)
      )
    )
    (_data_parallel_embedding_collection): EmbeddingCollection(
      (embeddings): ModuleDict(
        (action_weights): Embedding(11, 128)
      )
    )
  )
  (_hstu_block): HSTUBlock(
    (_preprocessor): HSTUBlockPreprocessor(
      (_positional_encoder): HSTUPositionalEncoder()
    )
    (_postprocessor): HSTUBlockPostprocessor()
    (_attention_layers): ModuleList(
      (0): HSTULayer(
        (_output_ln_dropout_mul): TPLayerNormMulDropout()
        (_linear_uvqk): TEColumnParallelLinear(in_features=128, out_features=2048, bias=True, TP=1)
        (_linear_proj): TERowParallelLinear(in_features=512, out_features=128, bias=False, TP=1)
        (_attn_func): FusedHSTUAttention()
      )
    )
  )
  (_mlp): MLP(
    (_mlp): Sequential(
      (0): Linear(in_features=128, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=10, bias=True)
      (3): Identity()
    )
  )
  (_loss_module): MultiTaskLossModule(
    (_loss_modules): CrossEntropyLoss()
  )
  (_metric_module): MultiClassificationTaskMetric(
    (_eval_metrics_modules): ModuleList(
      (0): MulticlassAUROC()
    )
  )
)


table name |          | memory(MB) |             |         | hbm(MB)/cuda:0 |             |          |  dram(MB) |            
---------- | -------- | ---------- | ----------- | ------- | -------------- | ----------- | -------- | --------- | -----------
           | total    | embedding  | optim_state | total   | embedding      | optim_state | total    | embedding | optim_state
---------- | -------- | ---------- | ----------- | ------- | -------------- | ----------- | -------- | --------- | -----------
movie_id   | 25165824 | 8388608    | 16777216    | 5000000 | 1666666        | 3333333     | 20165824 | 6721941   | 13443882   
user_id    | 25165824 | 8388608    | 16777216    | 5000000 | 1666666        | 3333333     | 20165824 | 6721941   | 13443882   
[2025-08-13 14:13:52] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-13 14:13:57.182577] [SequenceDataset] Filtered samples with short sequences: 4485 removed, 134008 remaining.
[2025-08-13 14:14:01.763026] [SequenceDataset] Filtered samples with short sequences: 4485 removed, 134008 remaining.
[2025-08-13 14:14:01.764106] model initialization done, start training. Free cuda memory: 14448.44 MB
[2025-08-13 14:14:01.765600] =========Training without time window============
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-08-13 14:14:10.229165] [train] [iter 9, tokens 25600, elapsed_time 8027.96 ms, achieved FLOPS 0.23 TFLOPS]: loss 2.008040
[2025-08-13 14:14:24.748061] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.634306
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-08-13 14:14:25.105576] [train] [iter 19, tokens 25600, elapsed_time 15311.49 ms, achieved FLOPS 0.12 TFLOPS]: loss 1.808315
[2025-08-13 14:14:33.764141] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.633158
[2025-08-13 14:14:34.136168] [train] [iter 29, tokens 25600, elapsed_time 9030.61 ms, achieved FLOPS 0.21 TFLOPS]: loss 1.741049
[2025-08-13 14:14:42.811499] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.703616
[2025-08-13 14:14:43.172885] [train] [iter 39, tokens 25600, elapsed_time 9036.67 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.640804
[2025-08-13 14:14:51.901977] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.732606
[2025-08-13 14:14:52.312237] [train] [iter 49, tokens 25600, elapsed_time 9139.30 ms, achieved FLOPS 0.29 TFLOPS]: loss 1.669510
[2025-08-13 14:15:01.046581] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.746771
[2025-08-13 14:15:01.430195] [train] [iter 59, tokens 25600, elapsed_time 9117.91 ms, achieved FLOPS 0.22 TFLOPS]: loss 1.656509
[2025-08-13 14:15:10.209043] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.755201
[2025-08-13 14:15:10.566496] [train] [iter 69, tokens 25600, elapsed_time 9136.27 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.627298
[2025-08-13 14:15:19.376309] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.764426
[2025-08-13 14:15:19.746605] [train] [iter 79, tokens 25600, elapsed_time 9180.05 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.596683
[2025-08-13 14:15:29.652850] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.769726
[2025-08-13 14:15:30.037002] [train] [iter 89, tokens 25600, elapsed_time 10290.34 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.639594
[2025-08-13 14:15:38.805849] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.775230
[2025-08-13 14:15:39.190018] [train] [iter 99, tokens 25600, elapsed_time 9152.95 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.661188
[2025-08-13 14:15:47.919479] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.779227
[2025-08-13 14:15:48.304792] [train] [iter 109, tokens 25600, elapsed_time 9114.73 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.570540
[2025-08-13 14:15:57.086548] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.784419
[2025-08-13 14:15:57.486780] [train] [iter 119, tokens 25600, elapsed_time 9181.92 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.561633
[2025-08-13 14:16:06.225980] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.786081
[2025-08-13 14:16:06.625867] [train] [iter 129, tokens 25600, elapsed_time 9139.03 ms, achieved FLOPS 0.19 TFLOPS]: loss 1.563152
[2025-08-13 14:16:15.377934] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.786450
[2025-08-13 14:16:15.769614] [train] [iter 139, tokens 25600, elapsed_time 9143.72 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.587614
[2025-08-13 14:16:24.519474] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.789545
[2025-08-13 14:16:24.906557] [train] [iter 149, tokens 25600, elapsed_time 9136.89 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.531298
[2025-08-13 14:16:33.953168] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.792456
[2025-08-13 14:16:34.378858] [train] [iter 159, tokens 25600, elapsed_time 9472.09 ms, achieved FLOPS 0.22 TFLOPS]: loss 1.541293
[2025-08-13 14:16:43.384360] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.792712
[2025-08-13 14:16:43.849708] [train] [iter 169, tokens 25600, elapsed_time 9470.75 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.583291
[2025-08-13 14:16:54.741819] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.794805
[2025-08-13 14:16:55.212018] [train] [iter 179, tokens 25600, elapsed_time 11362.23 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.497173
[2025-08-13 14:17:04.333179] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.795721
[2025-08-13 14:17:04.714351] [train] [iter 189, tokens 25600, elapsed_time 9502.48 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.619257
[2025-08-13 14:17:14.703464] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.797051
[2025-08-13 14:17:15.129869] [train] [iter 199, tokens 25600, elapsed_time 10415.27 ms, achieved FLOPS 0.21 TFLOPS]: loss 1.576840
[2025-08-13 14:17:24.566710] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.797626
[2025-08-13 14:17:24.957552] [train] [iter 209, tokens 25600, elapsed_time 9827.81 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.572742
[2025-08-13 14:17:33.766952] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.798718
[2025-08-13 14:17:34.221438] [train] [iter 219, tokens 25600, elapsed_time 9263.78 ms, achieved FLOPS 0.27 TFLOPS]: loss 1.559829
[2025-08-13 14:17:43.004579] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.799886
[2025-08-13 14:17:43.439471] [train] [iter 229, tokens 25600, elapsed_time 9217.98 ms, achieved FLOPS 0.24 TFLOPS]: loss 1.489769
[2025-08-13 14:17:52.446747] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.800897
[2025-08-13 14:17:52.850862] [train] [iter 239, tokens 25600, elapsed_time 9411.31 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.576145
[2025-08-13 14:18:01.720691] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.802331
[2025-08-13 14:18:02.134614] [train] [iter 249, tokens 25600, elapsed_time 9283.62 ms, achieved FLOPS 0.21 TFLOPS]: loss 1.559958
[2025-08-13 14:18:12.102067] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.802777
[2025-08-13 14:18:12.515958] [train] [iter 259, tokens 25600, elapsed_time 10381.42 ms, achieved FLOPS 0.21 TFLOPS]: loss 1.550345
[2025-08-13 14:18:21.183048] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.803967
[2025-08-13 14:18:21.583903] [train] [iter 269, tokens 25600, elapsed_time 9067.89 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.540799
[2025-08-13 14:18:30.293146] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.803722
[2025-08-13 14:18:30.669828] [train] [iter 279, tokens 25600, elapsed_time 9085.83 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.494294
[2025-08-13 14:18:39.399437] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.803748
[2025-08-13 14:18:39.802910] [train] [iter 289, tokens 25600, elapsed_time 9132.91 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.617859
[2025-08-13 14:18:48.630576] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.804655
[2025-08-13 14:18:49.056864] [train] [iter 299, tokens 25600, elapsed_time 9254.03 ms, achieved FLOPS 0.23 TFLOPS]: loss 1.543091
[2025-08-13 14:18:57.840468] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.805048
[2025-08-13 14:18:58.241363] [train] [iter 309, tokens 25600, elapsed_time 9184.44 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.536071
[2025-08-13 14:19:07.093468] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.805479
[2025-08-13 14:19:07.506201] [train] [iter 319, tokens 25600, elapsed_time 9264.81 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.534800
[2025-08-13 14:19:17.837832] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.805827
[2025-08-13 14:19:18.315803] [train] [iter 329, tokens 25600, elapsed_time 10809.34 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.522319
[2025-08-13 14:19:27.944955] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.806181
[2025-08-13 14:19:28.335836] [train] [iter 339, tokens 25600, elapsed_time 10020.16 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.504443
[2025-08-13 14:19:37.577525] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.806691
[2025-08-13 14:19:38.054036] [train] [iter 349, tokens 25600, elapsed_time 9717.97 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.533561
[2025-08-13 14:19:47.730613] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.807469
[2025-08-13 14:19:48.134065] [train] [iter 359, tokens 25600, elapsed_time 10080.12 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.574585
[2025-08-13 14:19:56.862916] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.807181
[2025-08-13 14:19:57.268304] [train] [iter 369, tokens 25600, elapsed_time 9134.16 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.520081
[2025-08-13 14:20:05.986457] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.807708
[2025-08-13 14:20:06.361484] [train] [iter 379, tokens 25600, elapsed_time 9093.19 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.481504
[2025-08-13 14:20:15.108264] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.807993
[2025-08-13 14:20:15.482766] [train] [iter 389, tokens 25600, elapsed_time 9121.19 ms, achieved FLOPS 0.14 TFLOPS]: loss 1.520774
[2025-08-13 14:20:24.208409] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.808097
[2025-08-13 14:20:24.593559] [train] [iter 399, tokens 25600, elapsed_time 9110.72 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.521747
[2025-08-13 14:20:33.344374] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.808543
[2025-08-13 14:20:33.749704] [train] [iter 409, tokens 25600, elapsed_time 9156.00 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.509719
[2025-08-13 14:20:43.518338] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.808460
[2025-08-13 14:20:43.918631] [train] [iter 419, tokens 25600, elapsed_time 10169.00 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.557430
[2025-08-13 14:20:52.771177] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.809020
[2025-08-13 14:20:53.172460] [train] [iter 429, tokens 25600, elapsed_time 9253.59 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.531801
[2025-08-13 14:21:02.030318] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.808665
[2025-08-13 14:21:02.443955] [train] [iter 439, tokens 25600, elapsed_time 9271.62 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.559261
[2025-08-13 14:21:11.274267] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.809334
[2025-08-13 14:21:11.681624] [train] [iter 449, tokens 25600, elapsed_time 9237.62 ms, achieved FLOPS 0.19 TFLOPS]: loss 1.603715
[2025-08-13 14:21:20.519649] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.809719
[2025-08-13 14:21:20.921559] [train] [iter 459, tokens 25600, elapsed_time 9239.75 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.446711
[2025-08-13 14:21:29.745307] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.810023
[2025-08-13 14:21:30.174395] [train] [iter 469, tokens 25600, elapsed_time 9252.72 ms, achieved FLOPS 0.26 TFLOPS]: loss 1.534591
[2025-08-13 14:21:41.153429] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.809796
[2025-08-13 14:21:41.589485] [train] [iter 479, tokens 25600, elapsed_time 11415.00 ms, achieved FLOPS 0.13 TFLOPS]: loss 1.480501
[2025-08-13 14:21:50.703549] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.809982
[2025-08-13 14:21:51.109023] [train] [iter 489, tokens 25600, elapsed_time 9519.56 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.539721
[2025-08-13 14:21:59.779668] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.809835
[2025-08-13 14:22:00.194327] [train] [iter 499, tokens 25600, elapsed_time 9085.34 ms, achieved FLOPS 0.21 TFLOPS]: loss 1.495710
[2025-08-13 14:22:08.899375] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.810226
[2025-08-13 14:22:09.293402] [train] [iter 509, tokens 25600, elapsed_time 9099.06 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.495356
[2025-08-13 14:22:18.006009] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.810578
[2025-08-13 14:22:18.404258] [train] [iter 519, tokens 25600, elapsed_time 9110.78 ms, achieved FLOPS 0.19 TFLOPS]: loss 1.568439
[2025-08-13 14:22:27.119407] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.810890
[2025-08-13 14:22:27.524216] [train] [iter 529, tokens 25600, elapsed_time 9119.91 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.474260
[2025-08-13 14:22:36.248000] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.810198
[2025-08-13 14:22:36.627816] [train] [iter 539, tokens 25600, elapsed_time 9103.53 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.556664
[2025-08-13 14:22:45.371033] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.810801
[2025-08-13 14:22:45.749211] [train] [iter 549, tokens 25600, elapsed_time 9121.31 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.481406
[2025-08-13 14:22:54.452780] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.811859
[2025-08-13 14:22:54.847512] [train] [iter 559, tokens 25600, elapsed_time 9098.28 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.474451
[2025-08-13 14:23:03.577674] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.811273
[2025-08-13 14:23:03.973145] [train] [iter 569, tokens 25600, elapsed_time 9125.56 ms, achieved FLOPS 0.19 TFLOPS]: loss 1.509623
[2025-08-13 14:23:12.697796] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.811415
[2025-08-13 14:23:13.153215] [train] [iter 579, tokens 25600, elapsed_time 9180.00 ms, achieved FLOPS 0.30 TFLOPS]: loss 1.516464
[2025-08-13 14:23:22.115173] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.811769
[2025-08-13 14:23:22.516754] [train] [iter 589, tokens 25600, elapsed_time 9363.50 ms, achieved FLOPS 0.18 TFLOPS]: loss 1.550540
[2025-08-13 14:23:32.578851] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.812113
[2025-08-13 14:23:32.936479] [train] [iter 599, tokens 25600, elapsed_time 10419.69 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.555066
[2025-08-13 14:23:41.779119] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.811343
[2025-08-13 14:23:42.136410] [train] [iter 609, tokens 25600, elapsed_time 9199.88 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.481719
[2025-08-13 14:23:50.957356] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.812246
[2025-08-13 14:23:51.311282] [train] [iter 619, tokens 25600, elapsed_time 9174.81 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.535188
[2025-08-13 14:24:00.125064] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.812285
[2025-08-13 14:24:00.484933] [train] [iter 629, tokens 25600, elapsed_time 9173.56 ms, achieved FLOPS 0.17 TFLOPS]: loss 1.489351
[2025-08-13 14:24:09.301364] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.812242
[2025-08-13 14:24:09.689311] [train] [iter 639, tokens 25600, elapsed_time 9204.31 ms, achieved FLOPS 0.23 TFLOPS]: loss 1.548521
[2025-08-13 14:24:18.492342] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.812658
[2025-08-13 14:24:18.833676] [train] [iter 649, tokens 25600, elapsed_time 9144.38 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.467665
[2025-08-13 14:24:27.674138] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.812337
[2025-08-13 14:24:28.055366] [train] [iter 659, tokens 25600, elapsed_time 9221.56 ms, achieved FLOPS 0.22 TFLOPS]: loss 1.444631
[2025-08-13 14:24:36.859998] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.812970
[2025-08-13 14:24:37.243091] [train] [iter 669, tokens 25600, elapsed_time 9187.69 ms, achieved FLOPS 0.20 TFLOPS]: loss 1.517307
[2025-08-13 14:24:46.045967] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.813098
[2025-08-13 14:24:46.389689] [train] [iter 679, tokens 25600, elapsed_time 9146.56 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.481256
[2025-08-13 14:24:55.223432] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.813354
[2025-08-13 14:24:55.577092] [train] [iter 689, tokens 25600, elapsed_time 9187.31 ms, achieved FLOPS 0.16 TFLOPS]: loss 1.491169
[2025-08-13 14:25:04.386858] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.812552
[2025-08-13 14:25:04.782733] [train] [iter 699, tokens 25600, elapsed_time 9205.56 ms, achieved FLOPS 0.24 TFLOPS]: loss 1.482404
[2025-08-13 14:25:13.550823] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.813187
[2025-08-13 14:25:13.940030] [train] [iter 709, tokens 25600, elapsed_time 9157.31 ms, achieved FLOPS 0.25 TFLOPS]: loss 1.499511
[2025-08-13 14:25:22.745286] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.813241
[2025-08-13 14:25:23.122978] [train] [iter 719, tokens 25600, elapsed_time 9182.81 ms, achieved FLOPS 0.21 TFLOPS]: loss 1.487124
[2025-08-13 14:25:31.929871] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.812991
[2025-08-13 14:25:32.300772] [train] [iter 729, tokens 25600, elapsed_time 9177.75 ms, achieved FLOPS 0.19 TFLOPS]: loss 1.541273
[2025-08-13 14:25:41.040736] [eval] [eval 40320 users]:
    Metrics.task0.AUC:0.813227
[2025-08-13 14:25:41.163139] ==================================================
[2025-08-13 14:25:41.163183] ==> SAVING CHECKPOINTS TO: /workspace/recsys-shared_data/ckpts/ml-20m/ranking/2025_08_13-14_13_36/final-iter733
[2025-08-13 14:25:41.163193] ==================================================
[yinj] Saving model type: <class 'torchrec.distributed.model_parallel.DistributedModelParallel'>
[yinj] Saving unwrapped_module type: <class 'model.ranking_gr.RankingGR'>
[2025-08-13 14:25:41.163846] dynamic module save dir /workspace/recsys-shared_data/ckpts/ml-20m/ranking/2025_08_13-14_13_36/final-iter733/dynamicemb_module
[yinj] [RANK 0] world_size = 1
DynamicEmb dump table movie_id from module _model_parallel_embedding_collection success!
[yinj] [RANK 0] world_size = 1
DynamicEmb dump table user_id from module _model_parallel_embedding_collection success!
[2025-08-13 14:25:41.871568] torch module save dir /workspace/recsys-shared_data/ckpts/ml-20m/ranking/2025_08_13-14_13_36/final-iter733/torch_module
[2025-08-13 14:25:42.059289] ==================================================
[2025-08-13 14:25:42.059339] ==> CHECKPOINTS SAVED SUCCESSFULLY âœ…
[2025-08-13 14:25:42.059351] ==================================================
[rank0]:[W813 14:25:43.115913504 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
