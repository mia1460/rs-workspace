[2025-08-10 09:54:26] [DEBUG] __main__ (pretrain_gr_ranking.py:37): successfully init logging
/usr/local/lib/python3.12/dist-packages/rapids_dask_dependency/dask_loader.py:36: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  return importlib.import_module(spec.name)
NOTE! Installing ujson may make loading annotations faster.
[2025-08-10 09:54:38.058495] distributed env initialization done. Free cuda memory: 23622.44 MB
RankingGR(
  (_embedding_collection): ShardedEmbedding(
    (_model_parallel_embedding_collection): EmbeddingCollection(
      (embeddings): ModuleDict(
        (video_id): Embedding(10000000, 128)
        (user_id): Embedding(10000000, 128)
      )
    )
    (_data_parallel_embedding_collection): EmbeddingCollection(
      (embeddings): ModuleDict(
        (user_active_degree): Embedding(10, 128)
        (follow_user_num_range): Embedding(9, 128)
        (fans_user_num_range): Embedding(10, 128)
        (friend_user_num_range): Embedding(8, 128)
        (register_days_range): Embedding(8, 128)
        (action_weights): Embedding(226, 128)
      )
    )
  )
  (_hstu_block): HSTUBlock(
    (_preprocessor): HSTUBlockPreprocessor(
      (_positional_encoder): HSTUPositionalEncoder()
    )
    (_postprocessor): HSTUBlockPostprocessor()
    (_attention_layers): ModuleList(
      (0-1): 2 x HSTULayer(
        (_output_ln_dropout_mul): TPLayerNormMulDropout()
        (_linear_uvqk): TEColumnParallelLinear(in_features=128, out_features=2048, bias=True, TP=1)
        (_linear_proj): TERowParallelLinear(in_features=512, out_features=128, bias=False, TP=1)
        (_attn_func): FusedHSTUAttention()
      )
    )
  )
  (_mlp): MLP(
    (_mlp): Sequential(
      (0): Linear(in_features=128, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=8, bias=True)
      (3): Identity()
    )
  )
  (_loss_module): MultiTaskLossModule(
    (_loss_modules): BCEWithLogitsLoss()
  )
  (_metric_module): MultiClassificationTaskMetric(
    (_eval_metrics_modules): ModuleList(
      (0-7): 8 x BinaryAUROC()
    )
  )
)


table name |          | memory(MB) |             |         | hbm(MB)/cuda:0 |             |          |  dram(MB) |            
---------- | -------- | ---------- | ----------- | ------- | -------------- | ----------- | -------- | --------- | -----------
           | total    | embedding  | optim_state | total   | embedding      | optim_state | total    | embedding | optim_state
---------- | -------- | ---------- | ----------- | ------- | -------------- | ----------- | -------- | --------- | -----------
video_id   | 25165824 | 8388608    | 16777216    | 5000000 | 1666666        | 3333333     | 20165824 | 6721941   | 13443882   
user_id    | 25165824 | 8388608    | 16777216    | 5000000 | 1666666        | 3333333     | 20165824 | 6721941   | 13443882   
[2025-08-10 09:54:53] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-10 09:54:53] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-10 09:54:53] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-10 09:54:53] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-10 09:54:53] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-10 09:54:53] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-10 09:54:54.157429] [SequenceDataset] Filtered samples with short sequences: 5947 removed, 19063 remaining.
[2025-08-10 09:54:54.788563] [SequenceDataset] Filtered samples with short sequences: 5947 removed, 19063 remaining.
[2025-08-10 09:54:54.789221] model initialization done, start training. Free cuda memory: 14446.44 MB
[2025-08-10 09:54:54.790761] =========Training without time window============
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-08-10 09:55:03.026336] [train] [iter 9, tokens 6400, elapsed_time 7793.45 ms, achieved FLOPS 0.03 TFLOPS]: loss 3.389252
/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:42: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score
  warnings.warn(*args, **kwargs)  # noqa: B028
[2025-08-10 09:55:07.849339] [eval] [eval 5728 users]:
    Metrics.task0.AUC:0.650656
    Metrics.task1.AUC:0.475834
    Metrics.task2.AUC:0.571263
    Metrics.task3.AUC:0.459477
    Metrics.task4.AUC:0.489864
    Metrics.task5.AUC:0.442564
    Metrics.task6.AUC:0.650538
    Metrics.task7.AUC:0.578906
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-08-10 09:55:08.111461] [train] [iter 19, tokens 6400, elapsed_time 5526.62 ms, achieved FLOPS 0.04 TFLOPS]: loss 1.976268
[2025-08-10 09:55:10.968452] [eval] [eval 5728 users]:
    Metrics.task0.AUC:0.679170
    Metrics.task1.AUC:0.533935
    Metrics.task2.AUC:0.593166
    Metrics.task3.AUC:0.533079
    Metrics.task4.AUC:0.555682
    Metrics.task5.AUC:0.489516
    Metrics.task6.AUC:0.681129
    Metrics.task7.AUC:0.580938
[2025-08-10 09:55:11.222351] [train] [iter 29, tokens 6400, elapsed_time 3110.89 ms, achieved FLOPS 0.07 TFLOPS]: loss 1.760706
[2025-08-10 09:55:14.075162] [eval] [eval 5728 users]:
    Metrics.task0.AUC:0.677293
    Metrics.task1.AUC:0.536900
    Metrics.task2.AUC:0.585060
    Metrics.task3.AUC:0.546072
    Metrics.task4.AUC:0.609357
    Metrics.task5.AUC:0.508768
    Metrics.task6.AUC:0.676539
    Metrics.task7.AUC:0.565258
[2025-08-10 09:55:14.343785] [train] [iter 39, tokens 6400, elapsed_time 3121.31 ms, achieved FLOPS 0.07 TFLOPS]: loss 1.546229
W0810 09:55:15.490000 342921 torch/distributed/elastic/agent/server/api.py:719] Received 2 death signal, shutting down workers
W0810 09:55:15.493000 342921 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 343033 closing signal SIGINT
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/recsys-examples/examples/hstu/pretrain_gr_ranking.py", line 204, in <module>
[rank0]:     main()
[rank0]:   File "/workspace/recsys-examples/examples/hstu/pretrain_gr_ranking.py", line 191, in main
[rank0]:     train_with_pipeline(
[rank0]:   File "/workspace/recsys-examples/examples/hstu/training/training.py", line 225, in train_with_pipeline
[rank0]:     evaluate(
[rank0]:   File "/workspace/recsys-examples/examples/hstu/training/training.py", line 62, in evaluate
[rank0]:     stateful_metric_module(logits, labels)
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/recsys-examples/examples/commons/utils/nvtx_op.py", line 163, in forward
[rank0]:     output = module(*args, **kwags)
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/recsys-examples/examples/hstu/modules/metrics/metric_modules.py", line 219, in forward
[rank0]:     _ = self._eval_metrics_modules[task_id](pred, target)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1747, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torchmetrics/metric.py", line 290, in forward
[rank0]:     self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)
[rank0]:                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torchmetrics/metric.py", line 358, in _forward_reduce_state_update
[rank0]:     batch_val = self.compute()
[rank0]:                 ^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torchmetrics/metric.py", line 593, in wrapped_func
[rank0]:     value = _squeeze_if_scalar(compute(*args, **kwargs))
[rank0]:                                ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torchmetrics/classification/auroc.py", line 122, in compute
[rank0]:     return _binary_auroc_compute(state, self.thresholds, self.max_fpr)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torchmetrics/functional/classification/auroc.py", line 88, in _binary_auroc_compute
[rank0]:     fpr, tpr, _ = _binary_roc_compute(state, thresholds, pos_label)
[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torchmetrics/functional/classification/roc.py", line 56, in _binary_roc_compute
[rank0]:     tps = torch.cat([torch.zeros(1, dtype=tps.dtype, device=tps.device), tps])
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: KeyboardInterrupt
[rank0]:[W810 09:55:15.685007136 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
W0810 09:55:21.730000 342921 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 343033 closing signal SIGTERM
Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 711, in run
    result = self._invoke_run(role)
             ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 870, in _invoke_run
    time.sleep(monitor_interval)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 342921 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 720, in run
    self._shutdown(e.sigval)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 372, in _shutdown
    self._pcontext.close(death_sig)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 572, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 909, in _close
    handler.proc.wait(time_to_wait)
  File "/usr/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/subprocess.py", line 2047, in _wait
    time.sleep(delay)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 342921 got signal: 2

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.6.0a0+df5bbc09d1.nv24.11', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 260, in launch_agent
    result = agent.run()
             ^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/metrics/api.py", line 137, in wrapper
    result = f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/api.py", line 725, in run
    self._shutdown()
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/agent/server/local_elastic_agent.py", line 372, in _shutdown
    self._pcontext.close(death_sig)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 572, in close
    self._close(death_sig=death_sig, timeout=timeout)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 909, in _close
    handler.proc.wait(time_to_wait)
  File "/usr/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.12/subprocess.py", line 2047, in _wait
    time.sleep(delay)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/api.py", line 84, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 342921 got signal: 2
