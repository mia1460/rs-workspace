[2025-08-07 09:01:12] [DEBUG] __main__ (pretrain_gr_ranking.py:30): successfully init logging
/usr/local/lib/python3.12/dist-packages/rapids_dask_dependency/dask_loader.py:36: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.
  return importlib.import_module(spec.name)
[2025-08-07 09:01:21.529495] distributed env initialization done. Free cuda memory: 23622.44 MB
RankingGR(
  (_embedding_collection): ShardedEmbedding(
    (_model_parallel_embedding_collection): EmbeddingCollection(
      (embeddings): ModuleDict(
        (video_id): Embedding(10000000, 128)
        (user_id): Embedding(10000000, 128)
      )
    )
    (_data_parallel_embedding_collection): EmbeddingCollection(
      (embeddings): ModuleDict(
        (user_active_degree): Embedding(10, 128)
        (follow_user_num_range): Embedding(9, 128)
        (fans_user_num_range): Embedding(10, 128)
        (friend_user_num_range): Embedding(8, 128)
        (register_days_range): Embedding(8, 128)
        (action_weights): Embedding(226, 128)
      )
    )
  )
  (_hstu_block): HSTUBlock(
    (_preprocessor): HSTUBlockPreprocessor(
      (_positional_encoder): HSTUPositionalEncoder()
    )
    (_postprocessor): HSTUBlockPostprocessor()
    (_attention_layers): ModuleList(
      (0-1): 2 x FusedHSTULayer()
    )
  )
  (_mlp): MLP(
    (_mlp): Sequential(
      (0): Linear(in_features=128, out_features=512, bias=True)
      (1): ReLU()
      (2): Linear(in_features=512, out_features=8, bias=True)
      (3): Identity()
    )
  )
  (_loss_module): MultiTaskLossModule(
    (_loss_modules): BCEWithLogitsLoss()
  )
  (_metric_module): MultiClassificationTaskMetric(
    (_eval_metrics_modules): ModuleList(
      (0-7): 8 x BinaryAUROC()
    )
  )
)


table name |          | memory(MB) |             |         | hbm(MB)/cuda:0 |             |          |  dram(MB) |            
---------- | -------- | ---------- | ----------- | ------- | -------------- | ----------- | -------- | --------- | -----------
           | total    | embedding  | optim_state | total   | embedding      | optim_state | total    | embedding | optim_state
---------- | -------- | ---------- | ----------- | ------- | -------------- | ----------- | -------- | --------- | -----------
video_id   | 25165824 | 8388608    | 16777216    | 5000000 | 1666666        | 3333333     | 20165824 | 6721941   | 13443882   
user_id    | 25165824 | 8388608    | 16777216    | 5000000 | 1666666        | 3333333     | 20165824 | 6721941   | 13443882   
[2025-08-07 09:01:39] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-07 09:01:39] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-07 09:01:39] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-07 09:01:39] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-07 09:01:39] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-07 09:01:39] [WARNING] torchrec.distributed.utils (utils.py:429): Sharding Type is data_parallel, caching params will be ignored
[2025-08-07 09:01:40.344720] [SequenceDataset] Filtered samples with short sequences: 5947 removed, 19063 remaining.
[2025-08-07 09:01:40.765620] [SequenceDataset.__init__]current_time is 1649479612164, earliest time is 1649476012164
[2025-08-07 09:01:42.092826] [SequenceDataset] Filtered samples with short sequences: 5947 removed, 19063 remaining.
[2025-08-07 09:01:42.156060] [SequenceDataset.__init__]current_time is 1649479563278, earliest time is 1649475963278
[2025-08-07 09:01:42.389168] model initialization done, start training. Free cuda memory: 14446.44 MB
[2025-08-07 09:01:42.390442] =========Traning time window 0============
[2025-08-07 09:01:42.390496] Time window (in ms): [1649479612164, 1649483212164], users cnt: 0, 
[2025-08-07 09:01:42.390721] before batched, max_train_iters: 0, n: 10, actual_train_iter: 0
[2025-08-07 09:01:42.949859] =========Traning time window 1============
[2025-08-07 09:01:42.949956] Time window (in ms): [1649483212164, 1649486812164], users cnt: 0, 
[2025-08-07 09:01:42.950154] before batched, max_train_iters: 0, n: 10, actual_train_iter: 0
[2025-08-07 09:01:43.502295] =========Traning time window 2============
[2025-08-07 09:01:43.502394] Time window (in ms): [1649486812164, 1649490412164], users cnt: 0, 
[2025-08-07 09:01:43.502593] before batched, max_train_iters: 0, n: 10, actual_train_iter: 0
[2025-08-07 09:01:44.043059] =========Traning time window 3============
[2025-08-07 09:01:44.043162] Time window (in ms): [1649490412164, 1649494012164], users cnt: 0, 
[2025-08-07 09:01:44.043396] before batched, max_train_iters: 0, n: 10, actual_train_iter: 0
[2025-08-07 09:01:44.586176] =========Traning time window 4============
[2025-08-07 09:01:44.586287] Time window (in ms): [1649494012164, 1649497612164], users cnt: 0, 
[2025-08-07 09:01:44.586546] before batched, max_train_iters: 0, n: 10, actual_train_iter: 0
[2025-08-07 09:01:45.137987] =========Traning time window 5============
[2025-08-07 09:01:45.138090] Time window (in ms): [1649497612164, 1649501212164], users cnt: 2, 
[2025-08-07 09:01:45.138333] before batched, max_train_iters: 1, n: 10, actual_train_iter: 0
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-08-07 09:01:52.579457] =========Traning time window 6============
[2025-08-07 09:01:52.579599] Time window (in ms): [1649501212164, 1649504812164], users cnt: 5, 
[2025-08-07 09:01:52.579965] before batched, max_train_iters: 1, n: 10, actual_train_iter: 1
[2025-08-07 09:01:55.110935] =========Traning time window 7============
[2025-08-07 09:01:55.111056] Time window (in ms): [1649504812164, 1649508412164], users cnt: 9, 
[2025-08-07 09:01:55.111279] before batched, max_train_iters: 1, n: 10, actual_train_iter: 2
[2025-08-07 09:01:55.706930] =========Traning time window 8============
[2025-08-07 09:01:55.707021] Time window (in ms): [1649508412164, 1649512012164], users cnt: 29, 
[2025-08-07 09:01:55.707207] before batched, max_train_iters: 1, n: 10, actual_train_iter: 3
[2025-08-07 09:01:58.216558] =========Traning time window 9============
[2025-08-07 09:01:58.216676] Time window (in ms): [1649512012164, 1649515612164], users cnt: 61, 
[2025-08-07 09:01:58.217031] before batched, max_train_iters: 2, n: 10, actual_train_iter: 4
[2025-08-07 09:01:58.837883] =========Traning time window 10============
[2025-08-07 09:01:58.838005] Time window (in ms): [1649515612164, 1649519212164], users cnt: 78, 
[2025-08-07 09:01:58.838296] before batched, max_train_iters: 3, n: 10, actual_train_iter: 6
[2025-08-07 09:01:59.440130] [train] [iter 9, tokens 3680, elapsed_time 16527.49 ms, achieved FLOPS 0.00 TFLOPS]: loss 3.904559
[2025-08-07 09:02:00.010514] =========Traning time window 11============
[2025-08-07 09:02:00.010601] Time window (in ms): [1649519212164, 1649522812164], users cnt: 82, 
[2025-08-07 09:02:00.010795] before batched, max_train_iters: 3, n: 10, actual_train_iter: 9
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-08-07 09:02:00.657590] =========Traning time window 12============
[2025-08-07 09:02:00.657699] Time window (in ms): [1649522812164, 1649526412164], users cnt: 61, 
[2025-08-07 09:02:00.657943] before batched, max_train_iters: 2, n: 10, actual_train_iter: 12
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-08-07 09:02:02.264996] =========Traning time window 13============
[2025-08-07 09:02:02.265131] Time window (in ms): [1649526412164, 1649530012164], users cnt: 54, 
[2025-08-07 09:02:02.265388] before batched, max_train_iters: 2, n: 10, actual_train_iter: 14
[2025-08-07 09:02:02.939338] =========Traning time window 14============
[2025-08-07 09:02:02.939435] Time window (in ms): [1649530012164, 1649533612164], users cnt: 42, 
[2025-08-07 09:02:02.939682] before batched, max_train_iters: 2, n: 10, actual_train_iter: 16
[2025-08-07 09:02:03.535235] =========Traning time window 15============
[2025-08-07 09:02:03.535351] Time window (in ms): [1649533612164, 1649537212164], users cnt: 23, 
[2025-08-07 09:02:03.535617] before batched, max_train_iters: 1, n: 10, actual_train_iter: 18
[2025-08-07 09:02:03.854132] [train] [iter 19, tokens 5240, elapsed_time 4645.45 ms, achieved FLOPS 0.01 TFLOPS]: loss 2.219938
[2025-08-07 09:02:04.583766] =========Traning time window 16============
[2025-08-07 09:02:04.583877] Time window (in ms): [1649537212164, 1649540812164], users cnt: 25, 
[2025-08-07 09:02:04.584131] before batched, max_train_iters: 1, n: 10, actual_train_iter: 19
/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py:825: UserWarning: fbgemm::dense_embedding_codegen_lookup_function: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/prints.py:42: UserWarning: The ``compute`` method of metric BinaryAUROC was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.
  warnings.warn(*args, **kwargs)  # noqa: B028
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/recsys-examples/examples/hstu/pretrain_gr_ranking.py", line 197, in <module>
[rank0]:     main()
[rank0]:   File "/workspace/recsys-examples/examples/hstu/pretrain_gr_ranking.py", line 184, in main
[rank0]:     train_with_pipeline(
[rank0]:   File "/workspace/recsys-examples/examples/hstu/training/training.py", line 291, in train_with_pipeline
[rank0]:     evaluate(
[rank0]:   File "/workspace/recsys-examples/examples/hstu/training/training.py", line 73, in evaluate
[rank0]:     eval_metric_dict = stateful_metric_module.compute()
[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/recsys-examples/examples/hstu/modules/metrics/metric_modules.py", line 238, in compute
[rank0]:     ] = eval_module.compute()
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torchmetrics/metric.py", line 593, in wrapped_func
[rank0]:     value = _squeeze_if_scalar(compute(*args, **kwargs))
[rank0]:                                ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torchmetrics/classification/auroc.py", line 121, in compute
[rank0]:     state = (dim_zero_cat(self.preds), dim_zero_cat(self.target)) if self.thresholds is None else self.confmat
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/usr/local/lib/python3.12/dist-packages/torchmetrics/utilities/data.py", line 34, in dim_zero_cat
[rank0]:     raise ValueError("No samples to concatenate")
[rank0]: ValueError: No samples to concatenate
[rank0]:[W807 09:02:05.945747786 ProcessGroupNCCL.cpp:1294] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
E0807 09:02:13.912000 702773 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 702956) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.6.0a0+df5bbc09d1.nv24.11', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
pretrain_gr_ranking.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-07_09:02:13
  host      : node182
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 702956)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
